# metaphorism
“metaphorism, metaphor-er, metaphorest...” ❦ word games and encoded thought experiments.

After a series of life-altering encounters with the delightfully meta “[How Does Work Work](http://www.inference.phy.cam.ac.uk/mackay/presentations/WORK/index.html)” (Mackay & Mackay), which turned out to be too informationally dense to digest at one point in time.

![](https://raw.githubusercontent.com/lmmx/shots/master/2016/Aug/metaphorest-thought.png)

My current best guess at what this approach is trying to achieve is to harness the [Dunning-Kruger effect](https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect) to deliberately attempt to set up some guides to structured thought, which then appear to be particularly recognisable when a confirmatory example is found.

In the case of _[intrinsically disordered proteins](https://en.wikipedia.org/wiki/Intrinsically_disordered_proteins)_, this sort of thinking allowed a really productive understanding of fundamental information-theoretical principles of entropy. These are utilised in names for IDPs as e.g. "molecular brush" etc.

Note that like the [memory palace](https://en.wikipedia.org/wiki/Method_of_loci) heuristic ("_method of loci_") for memorisation, such concepts stick in one's mind more easily.

They are also embellishable in a way much metadata is not, through the productive use of latent semantic layers.
  - for example, the 'metaphorest' idea clearly came simply from making the word 'metaphor' superlative and discovering a pun.
  - Later I noticed that it could be thought of more _productively_ as similar to '[random forest](https://en.wikipedia.org/wiki/Random_forest)' (RF)
    - conceptually, the machine learning (~'artificial intelligence') technique of RF creates an 'ensemble' [multitude] of [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning) and outputting the modal class [for classification] or mean prediction [for regression]

## Tree huggers beware!

> trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, because they have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.[3]:587–588 This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.
